{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03bbd939-b351-4ad5-a787-62fdd9431e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/wint3rmute/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from related_generator.post import Post\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba5f2b7e-9230-4da4-938d-e109fdc8a8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2cf9a1ca1847eca15445667f355796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/462 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import markdown\n",
    "import subprocess\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Post:\n",
    "    title: str\n",
    "    content: str\n",
    "    path: Path\n",
    "\n",
    "    related_links: list[str]\n",
    "    \n",
    "    tf_idf_position: list[float]\n",
    "    lsa_position: list[float]\n",
    "    \n",
    "    # To be filled by map generation\n",
    "    x: float = 0.0\n",
    "    y: float = 0.0\n",
    "\n",
    "    @classmethod\n",
    "    def from_path(cls, path: Path):\n",
    "        content_raw = path.read_text()\n",
    "        html_tree = BeautifulSoup(content_raw, features=\"html.parser\")\n",
    "        \n",
    "        # Get links from <nav>\n",
    "        content_raw = path.read_text()\n",
    "        html_tree = BeautifulSoup(content_raw, features=\"html.parser\")\n",
    "        nav_links = [l.attrs[\"href\"] for l in html_tree.nav.find_all(\"a\")]\n",
    "        \n",
    "        # Important: nav + footer MUST BE removed for later processing stages\n",
    "        html_tree.nav.decompose()\n",
    "        html_tree.footer.decompose()\n",
    "        \n",
    "        # Get 'Incoming' links\n",
    "        incoming_links = cls.find_incoming_links(path, html_tree) or []\n",
    "        \n",
    "        content = html_tree.text.replace(\"\\n\", \"\")\n",
    "        title = path.name\n",
    "\n",
    "        to_trim = content.rfind(\"Incoming:\")\n",
    "        if to_trim != -1:\n",
    "            content = content[to_trim:]\n",
    "       \n",
    "        related_links = list(set(nav_links) | set(incoming_links))\n",
    "        return cls(title=title, content=content, path=path, related_links=related_links, tf_idf_position=None, lsa_position=None)\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_incoming_links(path, html_tree) -> Optional[list]:\n",
    "        all_links = html_tree.find_all(\"a\")[::-1]\n",
    "        incoming_links = []\n",
    "        for link in all_links:\n",
    "            try:\n",
    "                incoming_links.append(link.attrs[\"href\"])\n",
    "            except Exception as e:\n",
    "                if link.text != \"link\" and link.text != \"external link\" and link.text != \"local link\":\n",
    "                    raise ValueError(link)\n",
    "                    raise e\n",
    "            \n",
    "            if \"class\" in link.attrs:\n",
    "                return incoming_links\n",
    "\n",
    "        # Executed if the links structure is atypical\n",
    "        return None\n",
    "    \n",
    "\n",
    "    def distance_to(self, post: \"Post\") -> float:\n",
    "        return math.sqrt((self.x - post.x) ** 2 + (self.y - post.y) ** 2)\n",
    "\n",
    "    def distance_tf_idf(self, post: \"Post\") -> float:\n",
    "        return np.linalg.norm(self.tf_idf_position - post.tf_idf_position)\n",
    "    \n",
    "    def distance_lsa(self, post: \"Post\") -> float:\n",
    "        return np.linalg.norm(self.lsa_position - post.lsa_position)\n",
    "\n",
    "def get_all_posts_oscean() -> list[Post]:\n",
    "    all_posts_paths = Path.glob(Path(\"/home/wint3rmute/code/misc/oscean/site/\"), \"**/*.html\")\n",
    "    all_posts_paths = [path for path in all_posts_paths]\n",
    "\n",
    "    with Pool() as p:\n",
    "        all_posts = list(tqdm(p.map(Post.from_path, all_posts_paths), total=len(all_posts_paths)))\n",
    "        return all_posts\n",
    "\n",
    "all_posts = get_all_posts_oscean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47c42293-4202-4b80-8dd6-bc50e25b4a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(text: str) -> list[str]:\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    words = word_tokenize(text) \n",
    "    \n",
    "    return [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d946f546-9319-4ce3-8ca1-39750cef8f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance of the SVD step: 40.8%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from itertools import count\n",
    "\n",
    "\n",
    "# Vectorizer to convert a collection of raw documents to a matrix of TF-IDF features\n",
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)\n",
    "\n",
    "# Learn vocabulary and idf, return term-document matrix.\n",
    "tfidf = vectorizer.fit_transform([post.content for post in all_posts])\n",
    "\n",
    "umap_result = umap.UMAP().fit_transform(\n",
    "    tfidf\n",
    ")\n",
    "\n",
    "lsa = make_pipeline(TruncatedSVD(n_components=50), Normalizer(copy=False))\n",
    "# t0 = time()\n",
    "X_lsa = lsa.fit_transform(tfidf)\n",
    "explained_variance = lsa[0].explained_variance_ratio_.sum()\n",
    "\n",
    "# print(f\"LSA done in {time() - t0:.3f} s\")\n",
    "print(f\"Explained variance of the SVD step: {explained_variance * 100:.1f}%\")\n",
    "\n",
    "\n",
    "for post, post_position, post_index in zip(all_posts, umap_result, count()):\n",
    "    post.x = post_position[0]\n",
    "    post.y = post_position[1]\n",
    "    post.tf_idf_position = tfidf[post_index]\n",
    "    post.lsa_position = X_lsa [post_index]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1a01d7b-0d42-468b-a1f2-c9c48c54f909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hits:   891\n",
      "all:    5784\n",
      "ratio:  0.15404564315352698\n"
     ]
    }
   ],
   "source": [
    "hits = 0\n",
    "all_comparisons = 0\n",
    "\n",
    "for post in all_posts[2:]:\n",
    "    posts_to_compare = [p for p in all_posts if p.title != post.title]\n",
    "    closest_posts = sorted(posts_to_compare, key=lambda p: p.distance_lsa(post))[:len(post.related_links)]\n",
    "    closest_posts_titles = [post.title for post in closest_posts]\n",
    "\n",
    "    for related in post.related_links:\n",
    "        if related in closest_posts_titles:\n",
    "            hits += 1\n",
    "\n",
    "        all_comparisons += 1\n",
    "        \n",
    "print(\"hits:  \", hits)\n",
    "print(\"all:   \", all_comparisons)\n",
    "print(\"ratio: \", hits/all_comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe06899-f773-4c4b-9bdb-a2c81d1378fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "from itertools import count\n",
    "import random\n",
    "for post, post_position, post_index in zip(all_posts, umap_result, count()):\n",
    "    plt.scatter(post_position[0], post_position[1])\n",
    "    \n",
    "    plt.annotate(post.title, post_position, post_position)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33e71b6-f8c5-44d3-be1a-6c4f15e9fa29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
